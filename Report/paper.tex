\documentclass{article}

\usepackage{times}
\usepackage{graphicx} 
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{icml2016} 
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{qtree}

\begin{document} 
\twocolumn[\icmltitle{Handwriting Recognition}
\icmlauthor{Jarre Knockaert}{Jarre.Knockaert@UGent.be}
\icmlauthor{Ruben Dedecker}{Ruben.Dedecker@UGent.be}

\vskip 0.3in
]

\begin{abstract} 
  
\end{abstract} 

\section{Introduction}

\section{Handwriting Recognition}

\subsection{Character Recognition}
The smallest distinguishable token in a text is a character. These handwritten characters are uniquely 
written by each individual and there's even variation when written by the same individual. These variations include the inclination of characters, i.e., the slant, orientation and size. These variations can be removed by preprocessing the image. We however take another, more general approach. Instead of adding rules to minimize variation we made a deep neural net which can recognise any character with a certain probability. The architecture of this deep neural network is described in \ref{sec:dnn}. In order to make the deep neural network recognise characters in images with these variations, we augmented the dataset to include images which have these variations. We discuss this in \ref{sec:preproc}. The dataset of handwritten character is described in \ref{sec:data}.

\subsubsection{Data}
\label{sec:data}
A good data dataset with many examples, enough variation, and which is balanced between classes is very important to train the neural network. We use the Chars74K dataset for this purpose. It has 62 classes (0-9, A-Z, a-z) and 3410 handwritten images. This dataset contains both very easily recognisable images of characters but also characters which are even hard to recognise for humans, as shown in figure \ref{fig:char}. 3410 input images is quite low for a neural network to be effectively trained. To overcome this problem we use some data augmentation techniques discussed in \ref{par:aug}. 

\begin{figure}
\begin{subfigure}{.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/bad_char1}
  \caption{Picture of a 'z' which resembles a 7.}
\end{subfigure}
\begin{subfigure}{.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/bad_char2}
  \caption{Picture of a 'z' which resembles a 3.}
\end{subfigure}
\begin{subfigure}{0.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/good_char}
  \caption{Picture of a 'z' which resembles a 3.}
\end{subfigure}
\caption{Examples of written 'z' characters.}
\label{fig:char}
\end{figure}


\subsubsection{Preliminary steps}
\label{sec:preproc}
We make use of 2 types of preprocessing steps: normalization of the data (\ref{par:norm}) and augmentation of the data (\ref{par:aug}). Normalization of data adjusts the original data and data augmentation takes the data and uses augmentation techniques to create variations of the original data.

\paragraph{Normalizing data} 
\label{par:norm}
Theoretically feeding normalized data to the neural network returns the same output as before. 
In reality however, it's better to normalize data to avoid getting stuck in local optima and to increase training speed of the model \cite{NormGoal}. We normalize the data by reducing the size from 1200x900x3 to 32x32x1 (width x height x number of color channels).

%TODO BINARIZATION -> INCREASE CONTRAST BETWEEN BACKGROUND AND FOREGROUND
%This might solve our problem that handwriting isn't recognised.

%TODO INVERTING -> We still need to check influence of inverting color values. 
%This surely helps for augmenting the data.

% Size / Contrast
\paragraph{Augmenting data}
\label{par:aug}
Data augmentation serves 2 purposes. It makes the dataset more robust to different kinds of handwriting as we add variations of the original data to the input data. The second purpose is to increase the size of our dataset. As our dataset only contains images of about 3400 images with handwritten characters, we need more data for the neural network to function to its fullest extend. Increasing the size of our dataset has great impact on the performance of the neural network as discussed in \ref{sec:expres}. Now we will describe the data augmentation techniques we used. For these techniques we based us on \cite{DataAug}. A more thorough explanation can be found there. 

\subparagraph{Adding noise to data}
We add a variant of each image which includes noise troughout the image with the purpose of reducing overfitting of the neural network \cite{DataNoise}. This noise is taken from the Gaussian distribution and summed with the original image.  
\subparagraph{Translations of data}
The dataset is also extended with variations in padding of characters in images. For this purpose we multiply every coordinate of the original image with the follow transformation matrix: 
\begin{equation}
        \begin{bmatrix}
                1 & 0 & t_x \\
                0 & 1 & t_y
        \end{bmatrix}
\end{equation}
where $t_x$ is the horizontal shift and $t_y$ is the vertical shift. 
\subparagraph{Rotations of data}
Rotations of the image are added to the dataset to make the network more robust to handwritings with different orientations. We rotate by -30 and 30 degrees. We use a builtin python function from the opencv2 library to create the transformation matrix. This matrix is equal to:
\begin{equation}
       \begin{bmatrix}
               \alpha & \beta & (1-\alpha)*center.x - \beta*center.y \\
               -\beta & \alpha & \beta*center.x + (1-\alpha)*center.y
       \end{bmatrix}
\end{equation}
where $\alpha = cos(angle)$ and $\beta = sin(angle)$. Center is the coordinate in our image around which is rotated and angle describes the amount of degrees to rotate in the clockwise direction. Translated images outside of the 32x32 frame are discarded.
\subparagraph{Scaling of data}
With the purpose of making the network more robust to different sizes of handwriting, we add different scalings of the data. We both scale in with and/or in height. The following transformation matrix scales the input image: 
\begin{equation}
       \begin{bmatrix}
               s_x & 0 & 0  \\
               0 & s_y & 0
       \end{bmatrix}
\end{equation}
where $s_x$ is the horizontal scaling factor and $s_y$ is the vertical scaling factor. 
\subparagraph{Shearing of data}
In order to deal with different kinds of slants we add sheared versions of the image to the dataset. 
Shearing displaces each point in a fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction \cite{Shear}. We used the following shear mapping to achieve this: 
\begin{equation}
        \begin{bmatrix}
                1 & s & 0 \\
                0 & 1 & 0
        \end{bmatrix}
\end{equation}
\subsubsection{Deep neural network}
\label{sec:dnn}
Humans have millions of connected neurons in the visual cortices which are capable of image processing. We, humans, can recognise most characters without any thought because of this complex neural network. For computers, character recognition is a more difficult task. We could try to write rule-based systems and define how every character should look like. This solution isn't flexible at all and is very hard to define. We use a neural network, inspired by how the brain processes images. These deep neural networks contain several layers of connected perceptrons, where each connection is defined by weight and each perceptron is defined by a bias. The perceptrons take an input, and calculate their output by calculating the activation function with the weights, input and bias. Images can be fed to the neural network, which then propagate trough several layers in the neural network and eventually produce as output the probabiliy that the input has a certain class, one of the possible characters. 

% I will add a discussion of the neural network once the neural network is finalized and not subject tot change. 
\subsection{Tokenization}

\subsection{Natural language processing}
To cope with some of the high error rates as discussed in section \ref{sec:expres}, there needs to be some kind of correction, in order to produce possibble written text even when a step of the larger algorithm fails. In order to do this we adapt popural natural language processing techniques as postprocessing steps. Natural language techniques can often be found in speech recognition, language generation and other systems, but this can be helpful here aswell to correct mistakes made in the previous steps of the algorithm. As a first step we match words against a dictionary to find out if they make sense, this is discussed in section \ref{sec:voc}. The second step checks if those given words make sense in the context, this is discussed in section \ref{sec:lm}.

\subsubsection{Vocabularium}
\label{sec:voc}
The neural network does not only produce which character is recognised, but rather a list of probabilities indicating the chance of an image to be a certain character. We can use all of this information in this postprocessing step instead of throwing that information away and naively returning the most likely character as the actual character. 
We make a tree, as shown in \ref{fig:wordtree}, where every node is a tuple with a word (sequence of characters) and a probability (likeliness of that word based on the probabilities of individual characters). The probability is calculated as a product of probability for each character.  
\begin{figure}
        %\Tree[.Y [.e [.s] [.5] [.l [.s] [.5]]]]
        %\Tree[.Y]
        \Tree [.{(Y, 0.9)} 
        [.{(Ye , 0.72)} {(Yes, 0.432)} {(Ye5 , 0.288)} ] 
        [.{(Yl , 0.18)} {(Yls, 0.108)} {(Yl5 , 0.072)} ]
            ] 
        \Tree [.{(T, 0.1)} 
        [.{(Te , 0.08)} {(Tes, 0.048)} {(Te5 , 0.032)} ] 
        [.{(Tl , 0.02)} {(Tls, 0.012)} {(Tl5 , 0.008)} ]
            ] 
\label{fig:wordtree} 
\caption{A list of the trees for the word 'Yes' with branching factor 2. The characters have the following probabilities: $P(Y)=0.9, P(T)=0.1, P(e)=0.8, P(l)=0.2, P(s)=0.6, P(5)=0.4$. Other characters have probabilities which are neglible.}
\end{figure} 
\subsubsection{Language model}
\label{sec:lm}
\section{Experiments and results}
\label{sec:expres}
\section{Conclusion}

\section{Future work}

\bibliography{paper}
\bibliographystyle{icml2016}


\end{document} 
