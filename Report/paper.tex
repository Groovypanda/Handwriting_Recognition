\documentclass{article}

\usepackage{times}
\usepackage{graphicx} 
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage[accepted]{icml2016} 
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{qtree}
\usepackage{listings}
\usepackage{titlesec}

\begin{document} 
\twocolumn[\icmltitle{Handwriting Recognition}
\icmlauthor{Jarre Knockaert}{Jarre.Knockaert@UGent.be}
\icmlauthor{Ruben Dedecker}{Ruben.Dedecker@UGent.be}

\vskip 0.3in
]

\begin{abstract} 
  
\end{abstract} 

\section{Introduction}

\section{Handwriting Recognition}

\subsection{Character Recognition}
The smallest distinguishable token in a text is a character. These handwritten characters are uniquely 
written by each individual and there's even variation when written by the same individual. These variations include the inclination of characters, i.e., the slant, orientation and size. These variations can be removed by preprocessing the image. We however take another, more general approach. Instead of adding rules to minimize variation we made a deep neural net which can recognise any character with a certain probability. The architecture of this deep neural network is described in \ref{sec:dnn}. In order to make the deep neural network recognise characters in images with these variations, we augmented the dataset to include images which have these variations. We discuss this in \ref{sec:preproc}. The dataset of handwritten character is described in \ref{sec:data}.

\subsubsection{Data}
\label{sec:data}
A good data dataset with many examples, enough variation, and which is balanced between classes is very important to train the neural network. We use the Chars74K dataset for this purpose. It has 62 classes (0-9, A-Z, a-z) and 3410 handwritten images. This dataset contains both very easily recognisable images of characters but also characters which are even hard to recognise for humans, as shown in figure \ref{fig:char}. 3410 input images is quite low for a neural network to be effectively trained. To overcome this problem we use some data augmentation techniques discussed in \ref{par:aug}. 

\begin{figure}
\begin{subfigure}{.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/bad_char1}
  \caption{Picture of a 'z' which resembles a 7.}
\end{subfigure}
\begin{subfigure}{.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/bad_char2}
  \caption{Picture of a 'z' which resembles a 3.}
\end{subfigure}
\begin{subfigure}{0.15\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/good_char}
  \caption{Picture of a clean 'z'.}
\end{subfigure}
\caption{Examples of written 'z' characters.}
\label{fig:char}
\end{figure}


\subsubsection{Preliminary steps}
\label{sec:preproc}
We make use of 2 types of preprocessing steps: normalization of the data and augmentation of the data. Normalization of data adjusts the original data and data augmentation takes the data and uses augmentation techniques to create variations of the original data.

\paragraph{Normalizing data} 
\label{par:norm}
Theoretically feeding normalized data to the neural network returns the same output as before. 
In reality however, it's better to normalize data to avoid getting stuck in local optima and to increase training speed of the model \cite{NormGoal}. We normalize the data by reducing the size from 1200x900x3 to 32x32x1 (width x height x number of color channels).

%TODO BINARIZATION -> INCREASE CONTRAST BETWEEN BACKGROUND AND FOREGROUND
%This might solve our problem that handwriting isn't recognised.

%TODO INVERTING -> We still need to check influence of inverting color values. 
%This surely helps for augmenting the data.

% Size / Contrast
\paragraph{Augmenting data}
\label{par:aug}
Data augmentation serves 2 purposes. It makes the dataset more robust to different kinds of handwriting as we add variations of the original data to the input data. The second purpose is to increase the size of our dataset. As our dataset only contains images of about 3400 images with handwritten characters, we need more data for the neural network to function to its fullest extend. Increasing the size of our dataset has great impact on the performance of the neural network as discussed in \ref{sec:expres}. Now we will describe the data augmentation techniques we used. For these techniques we based us on \cite{DataAug}. A more thorough explanation can be found there. 

\subparagraph{Adding noise to data}
We add a variant of each image which includes noise troughout the image with the purpose of reducing overfitting of the neural network \cite{DataNoise}. This noise is taken from the Gaussian distribution and summed with the original image.  
\subparagraph{Translations of data}
The dataset is also extended with variations in padding of characters in images. For this purpose we multiply every coordinate of the original image with the follow transformation matrix: 
\begin{equation}
        \begin{bmatrix}
                1 & 0 & t_x \\
                0 & 1 & t_y
        \end{bmatrix}
\end{equation}
where $t_x$ is the horizontal shift and $t_y$ is the vertical shift. 
\subparagraph{Rotations of data}
Rotations of the image are added to the dataset to make the network more robust to handwritings with different orientations. We rotate by -30 and 30 degrees. We use a builtin python function from the opencv2 library to create the transformation matrix. This matrix is equal to:
\begin{equation}
       \begin{bmatrix}
               \alpha & \beta & (1-\alpha)*center.x - \beta*center.y \\
               -\beta & \alpha & \beta*center.x + (1-\alpha)*center.y
       \end{bmatrix}
\end{equation}
where $\alpha = cos(angle)$ and $\beta = sin(angle)$. Center is the coordinate in our image around which is rotated and angle describes the amount of degrees to rotate in the clockwise direction. Translated images outside of the 32x32 frame are discarded.
\subparagraph{Scaling of data}
With the purpose of making the network more robust to different sizes of handwriting, we add different scalings of the data. We both scale in with and/or in height. The following transformation matrix scales the input image: 
\begin{equation}
       \begin{bmatrix}
               s_x & 0 & 0  \\
               0 & s_y & 0
       \end{bmatrix}
\end{equation}
where $s_x$ is the horizontal scaling factor and $s_y$ is the vertical scaling factor. 
\subparagraph{Shearing of data}
In order to deal with different kinds of slants we add sheared versions of the image to the dataset. 
Shearing displaces each point in a fixed direction, by an amount proportional to its signed distance from a line that is parallel to that direction \cite{Shear}. We used the following shear mapping to achieve this: 
\begin{equation}
        \begin{bmatrix}
                1 & s & 0 \\
                0 & 1 & 0
        \end{bmatrix}
\end{equation}
\subsubsection{Deep neural network}
\label{sec:dnn}
Humans have millions of connected neurons in the visual cortices which are capable of image processing. We, humans, can recognise most characters without any thought because of this complex neural network. For computers, character recognition is a more difficult task. We could try to write rule-based systems and define how every character should look like. This solution isn't flexible at all and is very hard to define. We use a neural network, inspired by how the brain processes images. These deep neural networks contain several layers of connected perceptrons, where each connection is defined by weight and each perceptron is defined by a bias. The perceptrons take an input, and calculate their output by calculating the activation function with the weights, input and bias. Images can be fed to the neural network, which then propagate trough several layers in the neural network and eventually produce as output the probabiliy that the input has a certain class, one of the possible characters. 

% I will add a discussion of the neural network once the neural network is finalized and not subject tot change. 
\subsection{Tokenization}

\subsection{Natural language processing}
To cope with some of the high error rates as discussed in section \ref{sec:expres}, there needs to be some kind of correction, in order to produce possibble written text even when a step of the larger algorithm fails. In order to do this we adapt popural natural language processing techniques as postprocessing steps. Natural language techniques can often be found in speech recognition, language generation and other systems, but this can be helpful here aswell to correct mistakes made in the previous steps of the algorithm. As a first step we match words against a dictionary to find out if they make sense, this is discussed in section \ref{sec:voc}. The second step checks if those given words make sense in the context, this is discussed in section \ref{sec:lm}.

\subsubsection{Vocabularium}
\label{sec:voc}
The neural network does not only produce which character is recognised, but rather a list of probabilities indicating the chance of an image to be a certain character. We can use all of this information in this postprocessing step instead of throwing that information away and naively returning the most likely character as the actual character. 
\begin{figure}
        \Tree [.{(Y, 0.9)} 
        [.{(Ye , 0.72)} {(Yes, 0.432)} {(Ye5 , 0.288)} ] 
        [.{(Yl , 0.18)} {(Yls, 0.108)} {(Yl5 , 0.072)} ]
            ] 
        \Tree [.{(T, 0.1)} 
        [.{(Te , 0.08)} {(Tes, 0.048)} {(Te5 , 0.032)} ] 
        [.{(Tl , 0.02)} {(Tls, 0.012)} {(Tl5 , 0.008)} ]
            ] 
\caption{A list of the trees for the word 'Yes' with branching factor 2. The characters have the following probabilities: $P(Y)=0.9, P(T)=0.1, P(e)=0.8, P(l)=0.2, P(s)=0.6, P(5)=0.4$. Other characters have probabilities which are neglible.}
\label{fig:wordtree} 
\end{figure} 
We make a tree, as shown in figure \ref{fig:wordtree}, where every node is a tuple with a word (sequence of characters) and a probability (likeliness of that word based on the probabilities of individual characters). The probability is calculated as a product of probability for each character. In order to keep the complexity low we only consider the 3 most probable characters. This will be the branching factor of every tree. There's a tree for every possible starting character. 
Now we have a list of words and their probabilities (every leaf). We can calculate the closest matches in the dictionary for each word. For this purpose we use the function \lstinline{get_close_matches} from the python library difflib. We use function to find close matches in the English dictionary and their corresponding score. We multiply this score with our previously calculated probability. This leaves us with a list of correct English words and a score for every word. If we only want to convert the image of one word into text, we can just return the word with the highest score. 
\subsubsection{Language model}
\label{sec:lm}
Besides checking the syntax of words, we can also look at the context. This is were language models are useful. We can check the previous words and calculate the likeness of a word to be the next word in the context. N-gram models are used to calculate probabilities given n-1 previous words. The probability is computed as $P(w_i | w_{i-n},...,w_{i-1})$. We use the Markov assumption here, we can approximate the probability of a word so the word only depends on the previous n-1 words. This probability is calculated as follows: 
\begin{equation}
        P(w_i | w_{i-n},...,w_{i-1}) = \frac{count(w_{i-n},...,w_{i-1},w_{i})}{count(w_{i-n},...,w_i)}
\end{equation}
where $w_i$ is the word of which we want to calculate the probability and $count(w_i,...w_{i+k})$ counts the amount of occurences of the sequence of words $w_i,...,w_{i+k}$. That amount of occurences is the same as counting the amount of n-grams with that sequence of words. An n-gram is a contiguous sequence of n items from a given sequence of text or speech. \cite{ngram}
We use the n-grams from the Google dataset for this purpose. Querying these datasets locally takes several minutes, and this happens for every word. Instead we use a Python library phrasefinder to make online queries to these corpora. This greatly increases speed but has the disadvantage of requiring internet connection. 
Now we can make queries such as "I like dogs / cats / sheep". Using the Google corpora probabilities are calculated with n-gram models for each of the 3 words. This gives us a probability for every word based on the context. 
Combining the results with the probabilities calculated in section \ref{sec:voc}, we can 
calculate a new score. The word with the highest score is accepted as written word. By iterating over all of the words, we now have converted the image with written text into text. In the next section we will
further discuss the results of these techniques and some experiments to check the performance and impact of optimizations. 
\section{Experiments and results}
%TODO
\subsection{Character Recognition}
\label{sec:expres}
We tested a lot of different configurations for the deep neural network in order to find a good functioning neural network given the low amounts of data. A problem which arrised when trying to test many diffeent configurations was the long time it takes to train a neural network. In order to keep these experiments from running several months, we added some small constraints. Next it's important to note that a lot of non-determinism is part of these neural networks. When running the neural network twice with the same configuration, it could have completely different results. 
In order to have meaningful results which run in a reasonable amount of time we ran every experiment 4 times but limited the amount of epochs to 200. This setup is important to note, as some results might be quite different if we run more tests or use more iterations. When discussing these results we take the best result of the 4 training results. We use this result as we are only interested in the potential of the neural network. If we'd let those experiments run for more iterations, they would reach the same accuracy because of the noise added to the neural network. The different configurations and results will be discussed next. 
\subsubsection{Experiments}
\paragraph{Preprocessing techniques}
Figure \ref{fig:preprocess} shows that preprocessing techniques have great influence on the performance of the neural network. We can see that the accuracy is much higher when using all of the preprocessing techniques.  We achieve 72\% accuracy with all of the preprocessing techniques versus 35\% without any preprocessing techniques, which is a huge difference. We can also see the individual impact of preprocessing in the figure. Adding shearing (64\%) to handle different amounts of slant, adding noise (58\%) to cope to make the recognition more robust and adding scaling (59\%) to handle different handwriting sizes was the most effective to increase performance. Adding rotations (40\%) and translations (50\%) was less effective (but still effective). Adding these preprocessing techniques increased the amount of data greatly from 3410 examples to 37000 examples. This improves the performance but also increases the training time of our network significantly. One remarkable result is that the training time of preprocessing with all techniques enabled, takes less long than some other results. When we look at the average however, the training time is the highest for this case. 
\paragraph{Filter amount}
% Not sure if I should discuss this, pretty boring results. 
\paragraph{Filter size}
In this experiment we check the influence of choosing different sizes of filters for the convolutional network. The filter size determines the size of our filters for convolving the input images. In figure \ref{fig:filtersize} we can see the time increases greatly, because more surrounding values have to be considered. The accuracy increases slightly when using bigger filters. Using different filter sizes allow other type of features to be extracted. 
% TODO: this is hard to explain..., maybe just leave this one out.
% Convolutation layers allow extracting features from the image. 
% Go over this again: good explanation: (See: http://www.deeplearningbook.org/)
% Pixels far away from eachother are less correlated. This is why we subsample filter responses.
\paragraph{Learning rate}
Figure \ref{fig:learningrate} shows the impact of learning rate on the training of our neural network. 
For this experiment we do have to note that we're using the Adam optimizer to train our neural network which computes individual learning rates for every parameter every training step. The learning rate which we pass is the initial learning rate. Choosing a high learning rate $10^-3$ allows the neural network to converge to a high accuracy (76 \%) very fast, but once reached the accuracy fails to grow any more.
This in contrast to higher learning rates which reach a good accuracy more slowly but find a more optimal solution. Learning rate $10^-4$ reaches an accuracy of 68\% after 200 iterations but is still growing. Learning rate $10^-5$ only reaches an accuracy of 5\% after 200 iterations but should find a better solution eventually than the other two configurations. 
\paragraph{Batch size}
In figure \ref{fig:batchsize} we can see the impact of choosing different batch sizes which are fed to the neural network. In order to increase to speed at which the neural network is trained, we feed batches of input images to the neural network instead of one image at a time. The backpropagation in order to calculate the weight adjustment only happens after every batch is fed. In the figure we can see bigger batch sizes increase the execution speed. However these differences are quites small. The configuration with batch size executes 200 iterations in one minute less than a configuration with batch size 128. These batch sizes also have an effect on our performance. When using small batch sizes backpropagation happens more frequently, which implies our weights will be more finetuned. The configuration with batch size 128 obtains an accuracy of 10\% higher than batch size 1024.
\paragraph{Weight decay}
In order to regulize the neural network we add weight decay to the network loss function in order to penalize big weights. The impact of different decays can be seen in \ref{fig:decay}. A high decay functions best (71\% accuracy) because we have a small amount of training examples. Without decay we only reach an accuracy of 65\%. 
%TODO: higher decay -> higher accuracy
\paragraph{Optimizer}
% Only useful optimizer is Adam. I'm not sure why, or if we shoul compare these at all. 
\paragraph{Dropout}
% Maybe redo this experiment? It's not clear at all at this time whether dropout has an actual effect on the accuracy. I believe it helps find a better global solution, but often the accuracy will be lower as we're only using half of the weights. Further research is required. 
\begin{figure}
\centering
    \includegraphics[width=\linewidth]{../CharacterRecognition/Graphs/preprocess.png}
    \caption{Impact of various preprocessing techniques.}
    \label{fig:preprocess}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{../CharacterRecognition/Graphs/filtersize.png}
    \caption{Impact of various filter sizes.}
    \label{fig:filtersize}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{../CharacterRecognition/Graphs/learningrate.png}
    \caption{Impact of different learning rates.}
    \label{fig:learningrate}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{../CharacterRecognition/Graphs/batchsize.png}
    \caption{Impact of different batch sizes.}
    \label{fig:batchsize}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{../CharacterRecognition/Graphs/decay.png}
    \caption{Impact of weight decay regularisation.}
    \label{fig:decay}
\end{figure}
\subsubsection{Final results}
% Add graph with several runs of the final network. Discuss which parameters we took. 
% Maybe some more visualisation of mispredicted characters? Oh this isn't part of char recognition...
% TODO TODO
\section{Conclusion}

\section{Future work}

\bibliography{paper}
\bibliographystyle{icml2016}


\end{document} 
